{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-written digit recognition with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root='~/data',train=True, download=True)\n",
    "mnist_test = datasets.MNIST(root='~/data',train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_data(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.length = len(data)\n",
    "        self.trans = transforms.ToTensor()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        inp = self.trans(self.data[index][0])\n",
    "        label = torch.tensor(self.data[index][1], dtype=torch.long)\n",
    "        return inp, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MNIST_data(mnist_train)\n",
    "test_data = MNIST_data(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/cnn_img.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Convnet,self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(1,8,5) # n1 = 8 # for convolution operation\n",
    "        self.conv_2 = nn.Conv2d(8,16,5) # n2 = 16\n",
    "        self.maxpool = nn.MaxPool2d(2,2) # for pooling operation\n",
    "        self.fc_3 = nn.Linear(16*4*4,24)\n",
    "        self.fc_4 = nn.Linear(24,10)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU() # Activation function\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        out = self.conv_1(inp)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.conv_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = out.reshape(inp.shape[0],-1) # flatten the output, first argument is batch-size\n",
    "        out = self.fc_3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc_4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cnn = Convnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(clf, train_data, test_data, epochs=20, learning_rate=0.0001):\n",
    "    optimizer = opt.Adam(clf.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        \n",
    "        for data, labels in DataLoader(train_data, batch_size=100, shuffle=True):\n",
    "            out = clf(data)\n",
    "            loss = criterion(out, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        score = evaluate(clf, test_data)\n",
    "        \n",
    "        print(f\"accuracy after epoch {_}: {score}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, test_data):\n",
    "    \n",
    "    true_labels = []\n",
    "    inf_labels = []\n",
    "    \n",
    "    for data, labels in DataLoader(test_data, batch_size=100):\n",
    "        out = clf(data)\n",
    "        cls = torch.argmax(F.softmax(out, dim=1), dim=1)\n",
    "        inf_labels.extend(cls.detach().numpy().tolist())\n",
    "        true_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "    \n",
    "    return accuracy_score(true_labels, inf_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after epoch 0: 0.7659\n",
      "accuracy after epoch 1: 0.8247\n",
      "accuracy after epoch 2: 0.8688\n",
      "accuracy after epoch 3: 0.8797\n",
      "accuracy after epoch 4: 0.908\n",
      "accuracy after epoch 5: 0.926\n",
      "accuracy after epoch 6: 0.935\n",
      "accuracy after epoch 7: 0.9393\n",
      "accuracy after epoch 8: 0.9435\n",
      "accuracy after epoch 9: 0.9451\n",
      "accuracy after epoch 10: 0.9487\n",
      "accuracy after epoch 11: 0.9501\n",
      "accuracy after epoch 12: 0.9545\n",
      "accuracy after epoch 13: 0.9567\n",
      "accuracy after epoch 14: 0.9584\n",
      "accuracy after epoch 15: 0.9587\n",
      "accuracy after epoch 16: 0.9608\n",
      "accuracy after epoch 17: 0.9643\n",
      "accuracy after epoch 18: 0.9662\n",
      "accuracy after epoch 19: 0.9642\n"
     ]
    }
   ],
   "source": [
    "train(clf_cnn, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs for text classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Design a CNN based text classifier model\n",
    "# We consider the task of sentiment classification on SST dataset\n",
    "# The dataset can be downloaded from https://nlp.stanford.edu/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The data is available in PTB format \n",
    "## We will first convert them into label,text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are not familiar with parsing, just use this function as black-box\n",
    "# It returns two lists - sentences - (the text to classify) and labels - (the corresponding sentiment labels 0/1)\n",
    "def get_data(fname):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with open(fname, encoding='utf8') as fs:\n",
    "        for line in fs:\n",
    "            label = 0 if int(line[1])<2 else 1\n",
    "            sentence = ' '.join(Tree.fromstring(line.strip()).leaves())\n",
    "            labels.append(label)\n",
    "            sentences.append(sentence)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, train_labels = get_data('Data/SST_trees/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents, test_labels = get_data('Data/SST_trees/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sents, val_labels = get_data('Data/SST_trees/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have to represent the words with some feature vectors, they cannot be directly used as input to the model\n",
    "## There are a few ways of doing it - \n",
    "#1. Character-level encoding - represent each character by a 1-hot encoding and stack each character on top of the \n",
    "# other in order og appearence to create a 2d matrix\n",
    "#2. Use some pretrained embedding of the words and then statck one over the other to form a 2d matrix as input\n",
    "#3. Start with some random initialization of the embeddings and then learn the embeddings along the way \n",
    "# This works for both character and word-based ones. The embedding module in Pytorch comes in handy in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will deploy a character-level CNN\n",
    "# We will also train in batches.. The problem with text is that not all datapoints will be of same length.. This would make \n",
    "# it difficult to fit them in batches due to size mismatch.\n",
    "# What we will do is set a max-length in terms of number of characters for each datapoint\n",
    "# If the text is shorter in length than this max-length we will pad with zeros\n",
    "# For the text with longer in length, the characters exceeding this max-length will be removed\n",
    "# The same techniqe could be adopted if you intend to deploy a word-level architecture.\n",
    "# The max-length would then be defined in terms of number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.printable #list of all characters which will form our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST_data(Dataset):\n",
    "    def __init__(self, train_sents, train_labels, max_length=512):\n",
    "        self.data = train_sents\n",
    "        self.labels = train_labels\n",
    "        self.max_length = max_length\n",
    "        self.length = len(train_labels)\n",
    "        self.vocabulary = string.printable\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[item]\n",
    "        label = self.labels[item]\n",
    "        inp = [self.vocabulary.find(c) + 1 for c in text] \n",
    "        # replace each chacter in the text with\n",
    "        # index 0 will be used as a special token for padding hence the indexing starts from 1\n",
    "        if len(inp)<=self.max_length:\n",
    "            inp.extend([0 for _ in range(self.max_length - len(inp))])\n",
    "        else:\n",
    "            inp = inp[:self.max_length]\n",
    "        \n",
    "        inp = torch.tensor(inp, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return inp, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SST_data(train_sents, train_labels)\n",
    "test_data = SST_data(test_sents, test_labels)\n",
    "val_data = SST_data(val_sents, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/CNN_text_classification.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_size, filters=64, max_len=512):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=0)\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=filters, kernel_size=(kernel_size, embedding_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(max_len-kernel_size+1)\n",
    "        self.linear = nn.Linear(filters, output_size)\n",
    "        self.sigmoid = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        x = self.embedding(inp)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x.squeeze(3))\n",
    "        x = x.squeeze(2)\n",
    "        return  self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(100, 200, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will reuse the train and evaluat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after epoch 0: 0.6121707538601272\n",
      "accuracy after epoch 1: 0.6439600363306085\n",
      "accuracy after epoch 2: 0.6357856494096276\n",
      "accuracy after epoch 3: 0.6657584014532243\n",
      "accuracy after epoch 4: 0.6748410535876476\n",
      "accuracy after epoch 5: 0.6775658492279746\n",
      "accuracy after epoch 6: 0.6821071752951862\n",
      "accuracy after epoch 7: 0.6712079927338783\n",
      "accuracy after epoch 8: 0.6857402361489555\n",
      "accuracy after epoch 9: 0.6839237057220708\n",
      "accuracy after epoch 10: 0.6766575840145322\n",
      "accuracy after epoch 11: 0.6902815622161671\n",
      "accuracy after epoch 12: 0.6748410535876476\n",
      "accuracy after epoch 13: 0.6911898274296094\n",
      "accuracy after epoch 14: 0.6893732970027248\n",
      "accuracy after epoch 15: 0.698455949137148\n",
      "accuracy after epoch 16: 0.6920980926430518\n",
      "accuracy after epoch 17: 0.6902815622161671\n",
      "accuracy after epoch 18: 0.6875567665758402\n",
      "accuracy after epoch 19: 0.6966394187102634\n"
     ]
    }
   ],
   "source": [
    "train(clf, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6705882352941176"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(clf, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture with different window sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(train_sents): # if we encounter a new word in test/val set we will replace it by <unk> token\n",
    "    # index 0 -> padding\n",
    "    # index 1 -> <unk>\n",
    "    w2i = {}\n",
    "    w2i['UNK'] = 1 # mapping each word to a unique id\n",
    "    \n",
    "    index = 2\n",
    "    \n",
    "    for sent in train_sents:\n",
    "        words = word_tokenize(sent)\n",
    "        for w in words:\n",
    "            if w not in w2i:\n",
    "                w2i[w] = index\n",
    "                index+=1\n",
    "    return w2i            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = create_vocab(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(w2i)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18269"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST_data(Dataset):\n",
    "    def __init__(self, train_sents, train_labels, max_length=128):\n",
    "        self.data = train_sents\n",
    "        self.labels = train_labels\n",
    "        self.max_length = max_length\n",
    "        self.length = len(train_labels)\n",
    "        self.vocabulary = string.printable\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[item]\n",
    "        label = self.labels[item]\n",
    "        inp = [w2i[w] if w in w2i else w2i['UNK'] for w in word_tokenize(text)] \n",
    "        if len(inp)<=self.max_length:\n",
    "            inp.extend([0 for _ in range(self.max_length - len(inp))])\n",
    "        else:\n",
    "            inp = inp[:self.max_length]\n",
    "        \n",
    "        inp = torch.tensor(inp, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return inp, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SST_data(train_sents, train_labels)\n",
    "test_data = SST_data(test_sents, test_labels)\n",
    "val_data = SST_data(val_sents, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/CNN_text_classification_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_size = [2,3,4], filters=2, max_len=128):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=filters, kernel_size=(kernel_size[0], embedding_dim))\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=filters, kernel_size=(kernel_size[1], embedding_dim))\n",
    "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=filters, kernel_size=(kernel_size[2], embedding_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(max_len-kernel_size[0]+1)\n",
    "        self.pool2 = nn.MaxPool1d(max_len-kernel_size[1]+1)\n",
    "        self.pool3 = nn.MaxPool1d(max_len-kernel_size[2]+1)\n",
    "        self.linear = nn.Linear(len(kernel_size)*filters, output_size)\n",
    "        self.sigmoid = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        x1 = self.pool1(self.relu(self.conv1(self.embedding(inp).unsqueeze(1))).squeeze(3)).squeeze(2)\n",
    "        x2 = self.pool2(self.relu(self.conv2(self.embedding(inp).unsqueeze(1))).squeeze(3)).squeeze(2)\n",
    "        x3 = self.pool3(self.relu(self.conv3(self.embedding(inp).unsqueeze(1))).squeeze(3)).squeeze(2)\n",
    "        out = torch.cat((x1, x2, x3),dim=1)\n",
    "        return  self.sigmoid(self.linear(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(vocab_size, 200, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at epoch 0: 0.6049046321525886\n",
      "Validation accuracy at epoch 1: 0.6103542234332425\n",
      "Validation accuracy at epoch 2: 0.6121707538601272\n",
      "Validation accuracy at epoch 3: 0.6176203451407811\n",
      "Validation accuracy at epoch 4: 0.6221616712079927\n",
      "Validation accuracy at epoch 5: 0.6330608537693007\n",
      "Validation accuracy at epoch 6: 0.6412352406902816\n",
      "Validation accuracy at epoch 7: 0.6421435059037239\n",
      "Validation accuracy at epoch 8: 0.6412352406902816\n",
      "Validation accuracy at epoch 9: 0.6485013623978202\n"
     ]
    }
   ],
   "source": [
    "train(clf, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
