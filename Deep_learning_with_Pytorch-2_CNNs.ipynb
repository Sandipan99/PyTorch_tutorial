{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-written digit recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root='~/data',train=True, download=True)\n",
    "mnist_test = datasets.MNIST(root='~/data',train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_data(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        s\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Design a CNN based text classifier model\n",
    "# We consider the task of sentiment classification on SST dataset\n",
    "# The dataset can be downloaded from https://nlp.stanford.edu/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The data is available in PTB format \n",
    "## We will first convert them into label,text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are not familiar with parsing, just use this function as black-box\n",
    "# It returns two lists - sentences - (the text to classify) and labels - (the corresponding sentiment labels 0/1)\n",
    "def get_data(fname):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with open(fname, encoding='utf8') as fs:\n",
    "        for line in fs:\n",
    "            label = 0 if int(line[1])<2 else 1\n",
    "            sentence = ' '.join(Tree.fromstring(line.strip()).leaves())\n",
    "            labels.append(label)\n",
    "            sentences.append(sentence)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, train_labels = get_data('Data/SST_trees/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents, test_labels = get_data('Data/SST_trees/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sents, val_labels = get_data('Data/SST_trees/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have to represent the words with some feature vectors, they cannot be directly used as input to the model\n",
    "## There are a few ways of doing it - \n",
    "#1. Character-level encoding - represent each character by a 1-hot encoding and stack each character on top of the \n",
    "# other in order og appearence to create a 2d matrix\n",
    "#2. Use some pretrained embedding of the words and then statck one over the other to form a 2d matrix as input\n",
    "#3. Start with some random initialization of the embeddings and then learn the embeddings along the way \n",
    "# This works for both character and word-based ones. The embedding module in Pytorch comes in handy in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will deploy a character-level CNN\n",
    "# We will also train in batches.. The problem with text is that not all datapoints will be of same length.. This would make \n",
    "# it difficult to fit them in batches due to size mismatch.\n",
    "# What we will do is set a max-length in terms of number of characters for each datapoint\n",
    "# If the text is shorter in length than this max-length we will pad with zeros\n",
    "# For the text with longer in length, the characters exceeding this max-length will be removed\n",
    "# The same techniqe could be adopted if you intend to deploy a word-level architecture.\n",
    "# The max-length would then be defined in terms of number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.printable #list of all characters which will form our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST_data(Dataset):\n",
    "    def __init__(self, train_sents, train_labels, max_length=512):\n",
    "        self.data = train_sents\n",
    "        self.labels = train_labels\n",
    "        self.max_length = max_length\n",
    "        self.length = len(train_labels)\n",
    "        self.vocabulary = string.printable\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[item]\n",
    "        label = self.labels[item]\n",
    "        inp = [self.vocabulary.find(c) + 1 for c in text] \n",
    "        # replace each chacter in the text with\n",
    "        # index 0 will be used as a special token for padding hence the indexing starts from 1\n",
    "        if len(inp)<=self.max_length:\n",
    "            inp.extend([0 for _ in range(self.max_length - len(inp))])\n",
    "        else:\n",
    "            inp = inp[:self.max_length]\n",
    "        \n",
    "        inp = torch.tensor(inp, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return inp, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SST_data(train_sents, train_labels)\n",
    "test_data = SST_data(test_sents, test_labels)\n",
    "val_data = SST_data(val_sents, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/CNN_text_classification.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_size, filters=64, max_len=512):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size+1, embedding_dim, padding_idx=0)\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=filters, kernel_size=(kernel_size, embedding_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(max_len-kernel_size+1)\n",
    "        self.linear = nn.Linear(filters, output_size)\n",
    "        self.sigmoid = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        x = self.embedding(inp)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x.squeeze(3))\n",
    "        x = x.squeeze(2)\n",
    "        return  self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(100, 200, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(clf, train_data, val_data, epochs=10, learning_rate=0.0001):\n",
    "    optimizer = opt.Adam(clf.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        \n",
    "        for data, labels in DataLoader(train_data, batch_size=32, shuffle=True):\n",
    "            out = clf(data)\n",
    "            loss = criterion(out, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        score = evaluate(clf, val_data)\n",
    "        print(f\"Validation accuracy at epoch {_}: {score}\")    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at epoch 0: 0.6148955495004541\n",
      "Validation accuracy at epoch 1: 0.6403269754768393\n",
      "Validation accuracy at epoch 2: 0.6512261580381471\n",
      "Validation accuracy at epoch 3: 0.662125340599455\n",
      "Validation accuracy at epoch 4: 0.6584922797456857\n",
      "Validation accuracy at epoch 5: 0.659400544959128\n",
      "Validation accuracy at epoch 6: 0.6793823796548593\n",
      "Validation accuracy at epoch 7: 0.6821071752951862\n",
      "Validation accuracy at epoch 8: 0.6911898274296094\n",
      "Validation accuracy at epoch 9: 0.6920980926430518\n"
     ]
    }
   ],
   "source": [
    "train(clf, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, test_data):\n",
    "    \n",
    "    true_labels = []\n",
    "    inf_labels = []\n",
    "    \n",
    "    for data, labels in DataLoader(test_data, batch_size=32):\n",
    "        out = clf(data)\n",
    "        cls = torch.argmax(F.softmax(out, dim=1), dim=1)\n",
    "        inf_labels.extend(cls.detach().numpy().tolist())\n",
    "        true_labels.extend(labels.numpy().tolist())\n",
    "    \n",
    "    return accuracy_score(true_labels, inf_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6705882352941176"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(clf, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture with different window sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(train_sents): # if we encounter a new word in test/val set we will replace it by <unk> token\n",
    "    # index 0 -> padding\n",
    "    # index 1 -> <unk>\n",
    "    w2i = {}\n",
    "    w2i['UNK'] = 1 # mapping each word to a unique id\n",
    "    \n",
    "    index = 2\n",
    "    \n",
    "    for sent in train_sents:\n",
    "        words = word_tokenize(sent)\n",
    "        for w in words:\n",
    "            if w not in w2i:\n",
    "                w2i[w] = index\n",
    "                index+=1\n",
    "    return w2i            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = create_vocab(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(w2i)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18269"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST_data(Dataset):\n",
    "    def __init__(self, train_sents, train_labels, max_length=128):\n",
    "        self.data = train_sents\n",
    "        self.labels = train_labels\n",
    "        self.max_length = max_length\n",
    "        self.length = len(train_labels)\n",
    "        self.vocabulary = string.printable\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[item]\n",
    "        label = self.labels[item]\n",
    "        inp = [w2i[w] if w in w2i else w2i['UNK'] for w in word_tokenize(text)] \n",
    "        if len(inp)<=self.max_length:\n",
    "            inp.extend([0 for _ in range(self.max_length - len(inp))])\n",
    "        else:\n",
    "            inp = inp[:self.max_length]\n",
    "        \n",
    "        inp = torch.tensor(inp, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return inp, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SST_data(train_sents, train_labels)\n",
    "test_data = SST_data(test_sents, test_labels)\n",
    "val_data = SST_data(val_sents, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/CNN_text_classification_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_size = [2,3,4], filters=2, max_len=128):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=filters, kernel_size=(kernel_size[0], embedding_dim))\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=filters, kernel_size=(kernel_size[1], embedding_dim))\n",
    "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=filters, kernel_size=(kernel_size[2], embedding_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(max_len-kernel_size[0]+1)\n",
    "        self.pool2 = nn.MaxPool1d(max_len-kernel_size[1]+1)\n",
    "        self.pool3 = nn.MaxPool1d(max_len-kernel_size[2]+1)\n",
    "        self.linear = nn.Linear(len(kernel_size)*filters, output_size)\n",
    "        self.sigmoid = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        x1 = self.pool1(self.relu(self.conv1(self.embedding(inp).unsqueeze(1))).squeeze(3)).squeeze(2)\n",
    "        x2 = self.pool2(self.relu(self.conv2(self.embedding(inp).unsqueeze(1))).squeeze(3)).squeeze(2)\n",
    "        x3 = self.pool3(self.relu(self.conv3(self.embedding(inp).unsqueeze(1))).squeeze(3)).squeeze(2)\n",
    "        out = torch.cat((x1, x2, x3),dim=1)\n",
    "        return  self.sigmoid(self.linear(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(vocab_size, 200, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at epoch 0: 0.6049046321525886\n",
      "Validation accuracy at epoch 1: 0.6103542234332425\n",
      "Validation accuracy at epoch 2: 0.6121707538601272\n",
      "Validation accuracy at epoch 3: 0.6176203451407811\n",
      "Validation accuracy at epoch 4: 0.6221616712079927\n",
      "Validation accuracy at epoch 5: 0.6330608537693007\n",
      "Validation accuracy at epoch 6: 0.6412352406902816\n",
      "Validation accuracy at epoch 7: 0.6421435059037239\n",
      "Validation accuracy at epoch 8: 0.6412352406902816\n",
      "Validation accuracy at epoch 9: 0.6485013623978202\n"
     ]
    }
   ],
   "source": [
    "train(clf, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
